from __future__ import annotations

import csv
import random
from dataclasses import dataclass
from typing import Any, Dict, List, Tuple

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from gensim.models.doc2vec import Doc2Vec
from torch.utils.data import DataLoader, Dataset


@dataclass
class TextDataset(Dataset):
    embeddings: List[List[float]]
    labels: List[int]

    def __len__(self) -> int:
        return len(self.embeddings)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        x = torch.tensor(self.embeddings[idx], dtype=torch.float)
        y = torch.tensor(self.labels[idx], dtype=torch.long)
        return x, y


class Classifier(nn.Module):
    def __init__(self, input_dim: int, hidden_dim: int, num_classes: int):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_dim, num_classes)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        out = self.fc1(x)
        out = self.relu(out)
        out = self.fc2(out)
        return out


@dataclass
class DataProcessor:
    csv_file: str
    doc2vec_model_path: str

    def load_csv_data(self) -> Tuple[List[List[str]], List[str]]:
        all_tokens = []
        all_labels = []
        print("ğŸ“¥ è¼‰å…¥ CSV è³‡æ–™...")

        with open(self.csv_file, "r", encoding="utf-8-sig") as f:
            reader = csv.DictReader(f)
            for row in reader:
                # è®€å–ä¸¦åˆ†å‰² tokens æ¬„ä½
                tokens = row["tokens"].split()
                if tokens:  # ç¢ºä¿æœ‰ tokens
                    all_tokens.append(tokens)
                    all_labels.append(row["board"])

        if not all_tokens:
            raise ValueError("æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ•¸æ“šï¼è«‹æª¢æŸ¥ CSV æª”æ¡ˆæ ¼å¼ã€‚")

        return all_tokens, all_labels

    def load_doc2vec_model(self) -> Tuple[Any, int]:
        print("ğŸ“¥ è¼‰å…¥é è¨“ç·´çš„ Doc2Vec æ¨¡å‹...")
        d2v_model = Doc2Vec.load(self.doc2vec_model_path)
        return d2v_model, d2v_model.vector_size

    def get_embeddings(
        self, all_tokens: List[List[str]], d2v_model: Doc2Vec, epochs: int = 50
    ) -> List[List[float]]:
        print("ğŸ”„ ç”¢ç”Ÿæ¯å€‹æ–‡ä»¶çš„ embeddings...")
        embeddings = []
        vector_size = d2v_model.vector_size
        zero_vector = np.zeros(vector_size)  # é è¨­çš„é›¶å‘é‡

        for tokens in all_tokens:
            # åªä¿ç•™åœ¨è©å½™è¡¨ä¸­çš„è©
            valid_words = [word for word in tokens if word in d2v_model.wv]

            if valid_words:
                # å¦‚æœæœ‰æœ‰æ•ˆçš„è©ï¼Œè¨ˆç®—å¹³å‡å€¼
                embedding = np.mean(
                    [d2v_model.wv[word] for word in valid_words], axis=0
                )
            else:
                # å¦‚æœæ²’æœ‰æœ‰æ•ˆçš„è©ï¼Œä½¿ç”¨é›¶å‘é‡
                embedding = zero_vector

            embeddings.append(embedding.tolist())

        return embeddings


@dataclass
class ModelTrainer:
    hidden_dim: int = 64
    num_epochs: int = 200
    batch_size: int = 64
    learning_rate: float = 0.01
    train_ratio: float = 0.8
    patience: int = 10  # åœæ­¢è¨“ç·´çš„è€å¿ƒå€¼ï¼Œå¦‚æœé€£çºŒ patience å€‹ epoch æ²’æœ‰æ”¹å–„å°±åœæ­¢

    def encode_labels(self, label_list: List[str]) -> Tuple[List[int], Dict[str, int]]:
        label_set = sorted(set(label_list))
        label2idx = {label: idx for idx, label in enumerate(label_set)}
        encoded = [label2idx[label] for label in label_list]
        return encoded, label2idx

    def split_data(
        self, embeddings: List[List[float]], labels: List[int]
    ) -> Tuple[List, List]:
        data = list(zip(embeddings, labels))
        random.shuffle(data)
        split_idx = int(self.train_ratio * len(data))
        train_data = data[:split_idx]
        test_data = data[split_idx:]

        print(f"\nğŸ“Š è³‡æ–™åˆ†å‰²:")
        print(f"è¨“ç·´è³‡æ–™: {len(train_data)} ç­† ({self.train_ratio*100:.0f}%)")
        print(f"æ¸¬è©¦è³‡æ–™: {len(test_data)} ç­† ({(1-self.train_ratio)*100:.0f}%)")

        return train_data, test_data

    def create_dataloaders(
        self, train_data: List, test_data: List
    ) -> Tuple[DataLoader, DataLoader]:
        train_embeddings, train_labels = zip(*train_data)
        test_embeddings, test_labels = zip(*test_data)

        train_dataset = TextDataset(train_embeddings, train_labels)
        test_dataset = TextDataset(test_embeddings, test_labels)

        train_loader = DataLoader(
            train_dataset, batch_size=self.batch_size, shuffle=True
        )
        test_loader = DataLoader(test_dataset, batch_size=self.batch_size)
        return train_loader, test_loader

    def evaluate_model(
        self, model: Classifier, data_loader: DataLoader, criterion: nn.Module
    ) -> Tuple[float, float, float]:
        model.eval()
        total_loss = 0.0
        total_samples = 0
        correct_top1 = 0
        correct_top2 = 0

        with torch.no_grad():
            for batch_x, batch_y in data_loader:
                outputs = model(batch_x)
                loss = criterion(outputs, batch_y)

                batch_size = batch_x.size(0)
                total_loss += loss.item() * batch_size
                total_samples += batch_size

                _, top1 = torch.max(outputs, 1)
                correct_top1 += (top1 == batch_y).sum().item()

                _, top2 = torch.topk(outputs, k=2, dim=1)
                correct_top2 += sum(batch_y[i] in top2[i] for i in range(batch_size))

        return (
            total_loss / total_samples,
            correct_top1 / total_samples,
            correct_top2 / total_samples,
        )

    def train_and_evaluate(
        self,
        model: Classifier,
        train_loader: DataLoader,
        test_loader: DataLoader,
        criterion: nn.Module,
        optimizer: optim.Optimizer,
    ) -> None:
        best_test_acc = 0.0
        no_improve_epochs = 0  # è¨˜éŒ„æ²’æœ‰æ”¹å–„çš„ epoch æ•¸

        for epoch in range(self.num_epochs):
            model.train()
            running_loss = 0.0
            total_samples = 0

            for batch_x, batch_y in train_loader:
                optimizer.zero_grad()
                outputs = model(batch_x)
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
                running_loss += loss.item() * batch_x.size(0)
                total_samples += batch_x.size(0)

            train_loss = running_loss / total_samples
            test_loss, top1_acc, top2_acc = self.evaluate_model(
                model, test_loader, criterion
            )

            print(f"\n===== Epoch {epoch+1}/{self.num_epochs} =====")
            print(f"ğŸ“Š è¨“ç·´è³‡æ–™å¹³å‡æå¤±: {train_loss:.6f}")
            print(f"ğŸ“Š æ¸¬è©¦è³‡æ–™å¹³å‡æå¤±: {test_loss:.6f}")
            print(f"ğŸ¯ Top-1 æº–ç¢ºç‡: {top1_acc:.6f}")
            print(f"ğŸ¯ Top-2 æº–ç¢ºç‡: {top2_acc:.6f}")

            # æª¢æŸ¥æ˜¯å¦æœ‰æ”¹å–„
            if top1_acc > best_test_acc:
                best_test_acc = top1_acc
                no_improve_epochs = 0  # é‡ç½®è¨ˆæ•¸å™¨
            else:
                no_improve_epochs += 1
                print(f"âš ï¸ é€£çºŒ {no_improve_epochs} å€‹ epoch æ²’æœ‰æ”¹å–„")

            # æª¢æŸ¥æ˜¯å¦éœ€è¦åœæ­¢è¨“ç·´
            if no_improve_epochs >= self.patience:
                print(f"\nğŸ›‘ é€£çºŒ {self.patience} å€‹ epoch æ²’æœ‰æ”¹å–„ï¼Œåœæ­¢è¨“ç·´")
                print(f"ğŸ¯ æœ€ä½³ Top-1 æº–ç¢ºç‡: {best_test_acc:.6f}")
                break

        if no_improve_epochs < self.patience:
            print(f"\nâœ… è¨“ç·´å®Œæˆï¼")
            print(f"ğŸ¯ æœ€ä½³ Top-1 æº–ç¢ºç‡: {best_test_acc:.6f}")


def execute_training():
    random.seed(42)
    torch.manual_seed(42)

    # è¨­å®šåƒæ•¸
    CSV_FILE = "data/tokenized_data.csv"
    DOC2VEC_MODEL_PATH = "doc2vec_model.model"

    # åˆå§‹åŒ–è™•ç†å™¨å’Œè¨“ç·´å™¨
    processor = DataProcessor(CSV_FILE, DOC2VEC_MODEL_PATH)
    trainer = ModelTrainer()

    # è¼‰å…¥å’Œè™•ç†è³‡æ–™
    all_tokens, all_labels = processor.load_csv_data()
    print(f"ğŸ“ ç¸½æ–‡ä»¶æ•¸: {len(all_tokens)}")

    encoded_labels, label2idx = trainer.encode_labels(all_labels)
    num_classes = len(label2idx)
    print(f"ğŸ“Š åˆ†é¡é¡åˆ¥æ•¸: {num_classes}")

    # è¼‰å…¥æ¨¡å‹å’Œç”¢ç”Ÿ embeddings
    d2v_model, vector_size = processor.load_doc2vec_model()
    embeddings = processor.get_embeddings(all_tokens, d2v_model, epochs=50)

    # æº–å‚™è¨“ç·´è³‡æ–™
    train_data, test_data = trainer.split_data(embeddings, encoded_labels)
    train_loader, test_loader = trainer.create_dataloaders(train_data, test_data)

    # å»ºç«‹å’Œè¨“ç·´æ¨¡å‹
    print("ğŸš€ é–‹å§‹è¨“ç·´åˆ†é¡å™¨...")
    classifier = Classifier(vector_size, trainer.hidden_dim, num_classes)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(classifier.parameters(), lr=trainer.learning_rate)

    trainer.train_and_evaluate(
        classifier, train_loader, test_loader, criterion, optimizer
    )


if __name__ == "__main__":
    execute_training()
